{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Hyperparameter Tuning for Pediatric Appendicitis Diagnosis (Part 1)\n",
    "\n",
    "This notebook provides a comprehensive evaluation of different machine learning models for pediatric appendicitis diagnosis, focusing on:\n",
    "- Model selection and comparison\n",
    "- Performance evaluation with various metrics\n",
    "- Hyperparameter tuning\n",
    "- Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data_processing.preprocess import load_data, handle_missing_values, optimize_memory\n",
    "\n",
    "# Set plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "Let's start by loading and preparing our dataset for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and preparing data...\")\n",
    "\n",
    "# Load synthetic data\n",
    "data_path = '../DATA/synthetic_appendicitis_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[['Age', 'Temperature', 'WBC', 'CRP', 'Pain_Duration', 'Neutrophil_Percent']]\n",
    "y = df['Appendicitis']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Class distribution in training set: {np.bincount(y_train)}\")\n",
    "print(f\"Class distribution in test set: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Selection and Initial Evaluation\n",
    "\n",
    "Let's evaluate various classification models to determine which ones perform best for our appendicitis diagnosis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42),\n",
    "    'CatBoost': cb.CatBoostClassifier(random_state=42, verbose=0)\n",
    "}\n",
    "\n",
    "# Function to evaluate and compare models\n",
    "def evaluate_models(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Evaluate multiple models and return their performance metrics.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "        cv_mean = np.mean(cv_scores)\n",
    "        cv_std = np.std(cv_scores)\n",
    "        \n",
    "        # Training time\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "            'AUC': auc,\n",
    "            'CV AUC Mean': cv_mean,\n",
    "            'CV AUC Std': cv_std,\n",
    "            'Training Time': train_time\n",
    "        })\n",
    "        \n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  AUC: {auc:.4f}\")\n",
    "        print(f\"  CV AUC: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
    "        print(f\"  Training Time: {train_time:.4f} seconds\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results_df = evaluate_models(models, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "results_df.set_index('Model')[metrics].plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/model_performance_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training time comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(results_df['Model'], results_df['Training Time'], color=sns.color_palette('viridis', len(results_df)))\n",
    "plt.title('Model Training Time Comparison')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/model_training_time_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detailed Evaluation of the Best Model\n",
    "\n",
    "Let's analyze the best performing model in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best model based on AUC\n",
    "best_model_idx = results_df['AUC'].idxmax()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name} with AUC {results_df.loc[best_model_idx, 'AUC']:.4f}\")\n",
    "\n",
    "# Generate detailed evaluation for the best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/confusion_matrix_best_model.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "plt.plot(fpr, tpr, lw=2, label=f'{best_model_name} (AUC = {roc_auc_score(y_test, y_prob):.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('../figures/roc_curve_best_model.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "average_precision = average_precision_score(y_test, y_prob)\n",
    "plt.plot(recall, precision, lw=2, label=f'{best_model_name} (AP = {average_precision:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.savefig('../figures/precision_recall_curve_best_model.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with Grid Search\n",
    "\n",
    "Now let's perform hyperparameter tuning on the best model to optimize its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPerforming hyperparameter tuning for the best model...\")\n",
    "\n",
    "# Define hyperparameter grid based on best model\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    }\n",
    "elif best_model_name == 'SVM':\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "        'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "    }\n",
    "elif best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "elif best_model_name == 'LightGBM':\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7, -1],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "elif best_model_name == 'CatBoost':\n",
    "    param_grid = {\n",
    "        'iterations': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'depth': [4, 6, 8],\n",
    "        'l2_leaf_reg': [1, 3, 5, 10],\n",
    "        'border_count': [32, 64, 128]\n",
    "    }\n",
    "\n",
    "# Setup grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    best_model,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "tuning_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nHyperparameter tuning completed in {tuning_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned model\n",
    "tuned_model = grid_search.best_estimator_\n",
    "y_pred_tuned = tuned_model.predict(X_test_scaled)\n",
    "y_prob_tuned = tuned_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
    "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned)\n",
    "auc_tuned = roc_auc_score(y_test, y_prob_tuned)\n",
    "\n",
    "print(\"\\nTuned Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_tuned:.4f}\")\n",
    "print(f\"Precision: {precision_tuned:.4f}\")\n",
    "print(f\"Recall: {recall_tuned:.4f}\")\n",
    "print(f\"F1 Score: {f1_tuned:.4f}\")\n",
    "print(f\"AUC: {auc_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs tuned model\n",
    "original_metrics = [\n",
    "    results_df.loc[best_model_idx, 'Accuracy'],\n",
    "    results_df.loc[best_model_idx, 'Precision'],\n",
    "    results_df.loc[best_model_idx, 'Recall'],\n",
    "    results_df.loc[best_model_idx, 'F1'],\n",
    "    results_df.loc[best_model_idx, 'AUC']\n",
    "]\n",
    "\n",
    "tuned_metrics = [accuracy_tuned, precision_tuned, recall_tuned, f1_tuned, auc_tuned]\n",
    "metrics_labels = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "x = np.arange(len(metrics_labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, original_metrics, width, label='Original Model')\n",
    "plt.bar(x + width/2, tuned_metrics, width, label='Tuned Model')\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title(f'Performance Comparison: Original vs Tuned {best_model_name}')\n",
    "plt.xticks(x, metrics_labels)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/tuned_model_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Tuned Model\n",
    "\n",
    "Let's save the best tuned model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned model\n",
    "model_path = f'../models/tuned_{best_model_name.lower().replace(\" \", \"_\")}_model.pkl'\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "joblib.dump(tuned_model, model_path)\n",
    "print(f\"\\nTuned model saved to {model_path}\")\n",
    "\n",
    "# Also save the scaler for future use\n",
    "scaler_path = '../models/standard_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "print(\"\\nPart 1 of model evaluation completed. See Part 2 for advanced evaluation techniques.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
