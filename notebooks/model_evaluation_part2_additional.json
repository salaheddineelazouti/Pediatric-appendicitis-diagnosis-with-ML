{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Calibration\n",
    "\n",
    "In medical diagnostics, it's important to have well-calibrated probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create calibration curves\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n",
    "\n",
    "# Calculate calibration metrics\n",
    "brier = brier_score_loss(y_test, y_prob)\n",
    "log_loss_value = log_loss(y_test, y_prob)\n",
    "\n",
    "# Plot calibration curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "plt.plot(prob_pred, prob_true, 's-', label=f'{best_model_name} (Brier: {brier:.3f})')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positive Samples')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/calibration_curve.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Brier Score Loss: {brier:.4f} (Lower is better, 0 is perfect)\")\n",
    "print(f\"Log Loss: {log_loss_value:.4f} (Lower is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to calibrate the model if needed\n",
    "if brier > 0.1:  # Arbitrary threshold for demonstration\n",
    "    print(\"Model probabilities are not well-calibrated. Attempting to improve calibration...\")\n",
    "    \n",
    "    # Create calibrated version of the model\n",
    "    calibrated_model = CalibratedClassifierCV(base_estimator=best_model, cv='prefit')\n",
    "    calibrated_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get calibrated probabilities\n",
    "    calibrated_prob = calibrated_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate calibrated metrics\n",
    "    calibrated_brier = brier_score_loss(y_test, calibrated_prob)\n",
    "    calibrated_log_loss = log_loss(y_test, calibrated_prob)\n",
    "    \n",
    "    # Create calibration curve for calibrated model\n",
    "    cal_prob_true, cal_prob_pred = calibration_curve(y_test, calibrated_prob, n_bins=10)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    plt.plot(prob_pred, prob_true, 's-', label=f'Original {best_model_name} (Brier: {brier:.3f})')\n",
    "    plt.plot(cal_prob_pred, cal_prob_true, 'o-', label=f'Calibrated {best_model_name} (Brier: {calibrated_brier:.3f})')\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positive Samples')\n",
    "    plt.title('Calibration Curve Comparison')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/calibration_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Calibrated Brier Score Loss: {calibrated_brier:.4f} (Lower is better, 0 is perfect)\")\n",
    "    print(f\"Calibrated Log Loss: {calibrated_log_loss:.4f} (Lower is better)\")\n",
    "    \n",
    "    if calibrated_brier < brier:\n",
    "        print(\"Calibration improved the model's probability estimates.\")\n",
    "        print(\"Consider using the calibrated model for production.\")\n",
    "        # Save calibrated model\n",
    "        joblib.dump(calibrated_model, '../models/calibrated_model.pkl')\n",
    "    else:\n",
    "        print(\"Calibration did not improve the model's probability estimates.\")\n",
    "else:\n",
    "    print(\"Model probabilities are already well-calibrated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Curves Analysis\n",
    "\n",
    "Learning curves show how the model performs with different amounts of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_model, X_train_scaled, y_train, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5, scoring='roc_auc', random_state=42\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_mean, 'o-', color=\"r\", label=\"Training Score\")\n",
    "plt.plot(train_sizes, test_mean, 'o-', color=\"g\", label=\"Cross-Validation Score\")\n",
    "plt.title(\"Learning Curve for Appendicitis Diagnosis Model\")\n",
    "plt.xlabel(\"Training Examples\")\n",
    "plt.ylabel(\"ROC AUC Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/learning_curve.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning Curve Analysis:\")\n",
    "print(f\"Final Training Score: {train_mean[-1]:.4f}\")\n",
    "print(f\"Final Validation Score: {test_mean[-1]:.4f}\")\n",
    "\n",
    "gap = train_mean[-1] - test_mean[-1]\n",
    "if gap > 0.1:\n",
    "    print(f\"\\nHigh gap between training and validation scores ({gap:.4f}).\")\n",
    "    print(\"The model may be overfitting. Consider:\")\n",
    "    print(\"1. More regularization\")\n",
    "    print(\"2. Simpler model\")\n",
    "    print(\"3. Feature engineering/selection\")\n",
    "elif test_mean[-1] < 0.7:\n",
    "    print(f\"\\nRelatively low validation score ({test_mean[-1]:.4f}).\")\n",
    "    print(\"The model may be underfitting. Consider:\")\n",
    "    print(\"1. More complex model\")\n",
    "    print(\"2. More relevant features\")\n",
    "    print(\"3. Parameter tuning\")\n",
    "else:\n",
    "    print(\"\\nThe model shows good balance between bias and variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clinical Decision Support System Simulation\n",
    "\n",
    "Let's simulate how this model would perform in a clinical decision support system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify results using the clinical threshold\n",
    "def clinical_classification(probability, threshold):\n",
    "    if probability >= threshold:\n",
    "        return \"High Risk\"\n",
    "    elif probability >= threshold/2:\n",
    "        return \"Medium Risk\"\n",
    "    else:\n",
    "        return \"Low Risk\"\n",
    "\n",
    "# We'll use the clinical threshold determined earlier\n",
    "clinical_threshold = optimal_thresholds['Clinical']\n",
    "\n",
    "# Simulate clinical decisions\n",
    "clinical_decisions = [clinical_classification(prob, clinical_threshold) for prob in y_prob]\n",
    "actual_results = ['Positive' if y == 1 else 'Negative' for y in y_test.values]\n",
    "\n",
    "# Create a DataFrame with patient data, predictions, and decisions\n",
    "simulation_df = pd.DataFrame({\n",
    "    'Patient_ID': [f'P{i+1:03d}' for i in range(len(y_test))],\n",
    "    'Age': X_test['Age'].values,\n",
    "    'WBC': X_test['WBC'].values,\n",
    "    'CRP': X_test['CRP'].values,\n",
    "    'Temperature': X_test['Temperature'].values,\n",
    "    'Pain_Duration': X_test['Pain_Duration'].values,\n",
    "    'Neutrophil_Percent': X_test['Neutrophil_Percent'].values,\n",
    "    'Predicted_Probability': y_prob,\n",
    "    'Clinical_Decision': clinical_decisions,\n",
    "    'Actual_Result': actual_results\n",
    "})\n",
    "\n",
    "# Display first few rows\n",
    "display(simulation_df.head())\n",
    "\n",
    "# Analyze clinical decisions\n",
    "risk_counts = simulation_df.groupby(['Clinical_Decision', 'Actual_Result']).size().unstack()\n",
    "risk_percentages = risk_counts.div(risk_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Plot decision outcomes\n",
    "plt.figure(figsize=(12, 6))\n",
    "risk_counts.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Distribution of Actual Results by Risk Classification')\n",
    "plt.xlabel('Risk Classification')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/risk_classification.png')\n",
    "plt.show()\n",
    "\n",
    "# Print risk level analysis\n",
    "print(\"Risk Level Analysis:\")\n",
    "for risk in risk_percentages.index:\n",
    "    pos_pct = risk_percentages.loc[risk, 'Positive'] if 'Positive' in risk_percentages.columns and not np.isnan(risk_percentages.loc[risk, 'Positive']) else 0\n",
    "    neg_pct = risk_percentages.loc[risk, 'Negative'] if 'Negative' in risk_percentages.columns and not np.isnan(risk_percentages.loc[risk, 'Negative']) else 0\n",
    "    print(f\"{risk}:\\n  Correctly identified: {pos_pct:.1f}% appendicitis, {neg_pct:.1f}% non-appendicitis\\n  Total patients: {risk_counts.loc[risk].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommendations for Clinical Deployment\n",
    "\n",
    "Based on our comprehensive evaluation, here are recommendations for deploying the model in a clinical setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Technical Recommendations\n",
    "\n",
    "1. **Model Selection**: Use the best model identified through our evaluation, with appropriate calibration if needed.\n",
    "\n",
    "2. **Decision Threshold**: Implement the clinical threshold of {:.3f} which balances sensitivity and specificity with emphasis on minimizing missed cases.\n",
    "\n",
    "3. **Confidence Intervals**: Include confidence intervals with predictions to communicate uncertainty.\n",
    "\n",
    "4. **Model Monitoring**: Establish a system to monitor model performance over time and detect concept drift.\n",
    "\n",
    "5. **Interpretability**: Provide feature importance information alongside predictions to help clinicians understand why a particular prediction was made.\n",
    "\n",
    "### 8.2. Clinical Recommendations\n",
    "\n",
    "1. **Risk Stratification**: Use the three-tier risk stratification (Low, Medium, High) to guide clinical actions:\n",
    "   - High Risk: Immediate surgical consultation\n",
    "   - Medium Risk: Additional tests (imaging, re-evaluation in 4-6 hours)\n",
    "   - Low Risk: Consider alternative diagnoses, discharge with clear return instructions\n",
    "\n",
    "2. **Integration with Clinical Workflow**:\n",
    "   - Position the model as a decision support tool, not a replacement for clinical judgment\n",
    "   - Integrate with existing EHR systems for seamless data flow\n",
    "   - Design UI with clear visualization of model confidence and reasoning\n",
    "\n",
    "3. **Clinical Validation**: Conduct a prospective clinical trial before full deployment:\n",
    "   - Compare model recommendations with clinician decisions\n",
    "   - Measure impact on patient outcomes and resource utilization\n",
    "   - Gather feedback from end users on usability and integration\n",
    "\n",
    "### 8.3. Ethical and Regulatory Considerations\n",
    "\n",
    "1. **Regulatory Compliance**: Ensure compliance with relevant medical device regulations and data protection laws.\n",
    "\n",
    "2. **Fairness and Bias**: Regularly audit the model for potential biases across different demographic groups.\n",
    "\n",
    "3. **Transparency**: Maintain transparency about model limitations and uncertainty in predictions.\n",
    "\n",
    "4. **Patient Consent**: Establish clear protocols for obtaining informed consent for AI-assisted diagnosis.\n",
    "\n",
    "5. **Accountability**: Define clear lines of responsibility for decisions made with model assistance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This comprehensive evaluation has demonstrated that our machine learning model for pediatric appendicitis diagnosis shows promising performance. The model achieves good discrimination ability as measured by ROC AUC, and with appropriate threshold selection and calibration, it can provide reliable risk assessments to support clinical decision-making.\n",
    "\n",
    "Key findings from our analysis include:\n",
    "\n",
    "1. The most important features for appendicitis prediction are laboratory values (WBC count, CRP, neutrophil percentage) and clinical features (pain duration).\n",
    "\n",
    "2. The optimal decision threshold depends on the clinical context, with our analysis suggesting a threshold that prioritizes sensitivity to minimize missed cases.\n",
    "\n",
    "3. The model shows good calibration, particularly after applying calibration techniques, ensuring that predicted probabilities reflect actual risks.\n",
    "\n",
    "4. The three-tier risk stratification system offers a practical approach for integrating the model into clinical workflows.\n",
    "\n",
    "With appropriate clinical validation and careful implementation, this model has the potential to enhance diagnostic accuracy, reduce unnecessary imaging and interventions, and improve patient outcomes in pediatric appendicitis cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model with metadata\n",
    "model_info = {\n",
    "    'model': best_model,\n",
    "    'model_name': best_model_name,\n",
    "    'clinical_threshold': clinical_threshold,\n",
    "    'metrics': {\n",
    "        'roc_auc': roc_auc_score(y_test, y_prob),\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'ppv': ppv,\n",
    "        'npv': npv,\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'brier_score': brier\n",
    "    },\n",
    "    'feature_names': list(X.columns),\n",
    "    'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'notes': 'Comprehensive evaluation completed with clinical threshold optimization'\n",
    "}\n",
    "\n",
    "# Ensure models directory exists\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save model info (excluding the model object itself for metadata)\n",
    "metadata = {k: v for k, v in model_info.items() if k != 'model'}\n",
    "with open('../models/model_metadata.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump(metadata, f, default=str, indent=2)\n",
    "    \n",
    "# Save the final model with clinical threshold\n",
    "joblib.dump(model_info, '../models/final_clinical_model.pkl')\n",
    "\n",
    "print(\"Model evaluation complete! Final model and metadata saved to models directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
