{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Optimization for Pediatric Appendicitis Model - Part 2\n",
    "\n",
    "This notebook continues our memory optimization strategies, focusing on model performance comparison and advanced techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries (from Part 1)\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "from memory_profiler import profile, memory_usage\n",
    "import psutil\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data_processing.preprocess import load_data, handle_missing_values, optimize_memory\n",
    "\n",
    "# Set plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Performance Comparison\n",
    "\n",
    "Let's compare model performance with and without memory optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (from memory_optimization_part1.ipynb)\n",
    "data_path = '../DATA/synthetic_appendicitis_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Basic optimization function\n",
    "def optimize_datatypes(df, verbose=True):\n",
    "    \"\"\"Optimize DataFrame data types to reduce memory usage\"\"\"\n",
    "    result = df.copy()\n",
    "    original_memory = result.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    \n",
    "    # Optimize integers\n",
    "    int_columns = result.select_dtypes(include=['int']).columns\n",
    "    for col in int_columns:\n",
    "        col_min = result[col].min()\n",
    "        col_max = result[col].max()\n",
    "        \n",
    "        # Find the appropriate int type\n",
    "        if col_min >= 0:\n",
    "            if col_max < 2**8:\n",
    "                result[col] = result[col].astype(np.uint8)\n",
    "            elif col_max < 2**16:\n",
    "                result[col] = result[col].astype(np.uint16)\n",
    "            elif col_max < 2**32:\n",
    "                result[col] = result[col].astype(np.uint32)\n",
    "        else:\n",
    "            if col_min > -2**7 and col_max < 2**7:\n",
    "                result[col] = result[col].astype(np.int8)\n",
    "            elif col_min > -2**15 and col_max < 2**15:\n",
    "                result[col] = result[col].astype(np.int16)\n",
    "            elif col_min > -2**31 and col_max < 2**31:\n",
    "                result[col] = result[col].astype(np.int32)\n",
    "    \n",
    "    # Optimize floats\n",
    "    float_columns = result.select_dtypes(include=['float']).columns\n",
    "    for col in float_columns:\n",
    "        result[col] = pd.to_numeric(result[col], downcast='float')\n",
    "    \n",
    "    # Optimize objects (strings)\n",
    "    categorical_threshold = 0.5  # Threshold for categorical conversion (50% unique values)\n",
    "    object_columns = result.select_dtypes(include=['object']).columns\n",
    "    for col in object_columns:\n",
    "        unique_count = len(result[col].unique())\n",
    "        total_count = len(result[col])\n",
    "        if unique_count / total_count < categorical_threshold:\n",
    "            result[col] = result[col].astype('category')\n",
    "    \n",
    "    # Calculate memory savings\n",
    "    optimized_memory = result.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    savings = original_memory - optimized_memory\n",
    "    savings_percent = (savings / original_memory) * 100\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Original memory usage: {original_memory:.2f} MB\")\n",
    "        print(f\"Optimized memory usage: {optimized_memory:.2f} MB\")\n",
    "        print(f\"Memory savings: {savings:.2f} MB ({savings_percent:.1f}%)\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for comparison\n",
    "print(\"Preparing datasets for model comparison...\")\n",
    "\n",
    "# Original dataset\n",
    "X_orig = df[['Age', 'Temperature', 'WBC', 'CRP', 'Pain_Duration', 'Neutrophil_Percent']]\n",
    "y_orig = df['Appendicitis']\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_orig, y_orig, test_size=0.3, random_state=42, stratify=y_orig\n",
    ")\n",
    "\n",
    "# Optimized dataset (data type optimization)\n",
    "df_opt = optimize_datatypes(df, verbose=False)\n",
    "X_opt = df_opt[['Age', 'Temperature', 'WBC', 'CRP', 'Pain_Duration', 'Neutrophil_Percent']]\n",
    "y_opt = df_opt['Appendicitis']\n",
    "X_train_opt, X_test_opt, y_train_opt, y_test_opt = train_test_split(\n",
    "    X_opt, y_opt, test_size=0.3, random_state=42, stratify=y_opt\n",
    ")\n",
    "\n",
    "# Feature-selected dataset (top 4 features)\n",
    "selector = SelectKBest(f_classif, k=4)\n",
    "X_train_fs = selector.fit_transform(X_train_opt, y_train_opt)\n",
    "X_test_fs = selector.transform(X_test_opt)\n",
    "selected_features = X_opt.columns[selector.get_support(indices=True)]\n",
    "print(f\"Selected features: {', '.join(selected_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to benchmark model training and evaluation\n",
    "def benchmark_model(X_train, X_test, y_train, y_test, description=\"\"):\n",
    "    \"\"\"Benchmark RandomForest model training and evaluation memory/time\"\"\"\n",
    "    # Collect garbage before benchmark\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nBenchmarking {description}\")\n",
    "    \n",
    "    # Get initial memory usage\n",
    "    mem_before = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "    \n",
    "    # Training time and memory\n",
    "    train_start = time.time()\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - train_start\n",
    "    \n",
    "    # Get memory after training\n",
    "    mem_after_train = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "    train_memory = mem_after_train - mem_before\n",
    "    \n",
    "    # Prediction time and performance\n",
    "    predict_start = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    predict_time = time.time() - predict_start\n",
    "    \n",
    "    # Get memory after prediction\n",
    "    mem_after_predict = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "    predict_memory = mem_after_predict - mem_after_train\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    # Get model size\n",
    "    model_size_mb = get_size(model) / (1024 * 1024)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
    "    print(f\"Training Time: {train_time:.4f} seconds\")\n",
    "    print(f\"Training Memory: {train_memory:.2f} MB\")\n",
    "    print(f\"Prediction Time: {predict_time:.4f} seconds\")\n",
    "    print(f\"Prediction Memory: {predict_memory:.2f} MB\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'description': description,\n",
    "        'model_size': model_size_mb,\n",
    "        'train_time': train_time,\n",
    "        'train_memory': train_memory,\n",
    "        'predict_time': predict_time,\n",
    "        'predict_memory': predict_memory,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define get_size function\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively find the size of objects in bytes\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark models with different memory optimization techniques\n",
    "results = []\n",
    "\n",
    "# Benchmark original dataset model\n",
    "results.append(benchmark_model(X_train_orig, X_test_orig, y_train_orig, y_test_orig, \"Original Dataset\"))\n",
    "\n",
    "# Benchmark optimized datatypes model\n",
    "results.append(benchmark_model(X_train_opt, X_test_opt, y_train_opt, y_test_opt, \"Optimized Datatypes\"))\n",
    "\n",
    "# Benchmark feature-selected model\n",
    "results.append(benchmark_model(X_train_fs, X_test_fs, y_train_opt, y_test_opt, \"Feature Selection (4 features)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparison of optimization techniques:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot memory metrics comparison\n",
    "plt.figure(figsize=(14, 7))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(results_df['description']))\n",
    "\n",
    "plt.bar(index, results_df['model_size'], bar_width, label='Model Size (MB)', color='#FF9999')\n",
    "plt.bar(index + bar_width, results_df['train_memory'], bar_width, label='Training Memory (MB)', color='#66B2FF')\n",
    "\n",
    "plt.xlabel('Optimization Technique')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.title('Memory Usage Comparison Across Optimization Techniques')\n",
    "plt.xticks(index + bar_width / 2, results_df['description'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance comparison\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results_df['description'], results_df['train_time'], color='#FF9999')\n",
    "plt.title('Training Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results_df['description'], results_df['auc'], color='#66B2FF')\n",
    "plt.title('Model Performance (AUC) Comparison')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0.8, 1.0)  # Adjust as needed based on your AUC scores\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimal Model Selection\n",
    "\n",
    "Let's analyze the trade-offs between memory usage and model performance to select the optimal approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate memory-performance efficiency\n",
    "results_df['memory_efficiency'] = results_df['auc'] / results_df['model_size']\n",
    "results_df['time_efficiency'] = results_df['auc'] / results_df['train_time']\n",
    "\n",
    "print(\"Memory-Performance Efficiency (AUC per MB):\")\n",
    "print(results_df[['description', 'memory_efficiency']].sort_values('memory_efficiency', ascending=False))\n",
    "\n",
    "print(\"\\nTime-Performance Efficiency (AUC per second):\")\n",
    "print(results_df[['description', 'time_efficiency']].sort_values('time_efficiency', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot efficiency metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "bars = plt.bar(results_df['description'], results_df['memory_efficiency'], color='#4CAF50')\n",
    "plt.title('Memory Efficiency (AUC per MB)')\n",
    "plt.ylabel('Efficiency')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.4f}', ha='center', va='bottom')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "bars = plt.bar(results_df['description'], results_df['time_efficiency'], color='#2196F3')\n",
    "plt.title('Time Efficiency (AUC per second)')\n",
    "plt.ylabel('Efficiency')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.4f}', ha='center', va='bottom')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving and Loading Optimized Models\n",
    "\n",
    "Let's train and save our most efficient model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the most memory-efficient model approach\n",
    "most_efficient_approach = results_df.loc[results_df['memory_efficiency'].idxmax()]\n",
    "print(f\"Most memory-efficient approach: {most_efficient_approach['description']}\")\n",
    "\n",
    "# Train and save the optimal model\n",
    "if most_efficient_approach['description'] == \"Feature Selection (4 features)\":\n",
    "    # Train on feature-selected data\n",
    "    optimal_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    optimal_model.fit(X_train_fs, y_train_opt)\n",
    "    \n",
    "    # Save feature selector and model\n",
    "    print(\"Saving feature selector and optimized model...\")\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    with open('../models/feature_selector.pkl', 'wb') as f:\n",
    "        pickle.dump(selector, f)\n",
    "    \n",
    "    with open('../models/memory_optimized_model.pkl', 'wb') as f:\n",
    "        pickle.dump(optimal_model, f)\n",
    "    \n",
    "    print(\"Feature selector and model saved successfully.\")\n",
    "    \n",
    "elif most_efficient_approach['description'] == \"Optimized Datatypes\":\n",
    "    # Train on optimized-datatypes data\n",
    "    optimal_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    optimal_model.fit(X_train_opt, y_train_opt)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving optimized model...\")\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    with open('../models/memory_optimized_model.pkl', 'wb') as f:\n",
    "        pickle.dump(optimal_model, f)\n",
    "    \n",
    "    print(\"Model saved successfully.\")\n",
    "else:\n",
    "    # Train on original data\n",
    "    optimal_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    optimal_model.fit(X_train_orig, y_train_orig)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    with open('../models/memory_optimized_model.pkl', 'wb') as f:\n",
    "        pickle.dump(optimal_model, f)\n",
    "    \n",
    "    print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved model and measure memory usage\n",
    "gc.collect()\n",
    "mem_before = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# Load the model\n",
    "with open('../models/memory_optimized_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "mem_after = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "loading_memory = mem_after - mem_before\n",
    "\n",
    "print(f\"Memory used to load the model: {loading_memory:.2f} MB\")\n",
    "print(f\"Loaded model size: {get_size(loaded_model) / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Best Practices\n",
    "\n",
    "Let's summarize our findings and recommend best practices for memory optimization in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Memory Optimization Best Practices ===\\n\")\n",
    "print(\"1. Datatype Optimization:\")\n",
    "print(\"   - Use the smallest possible datatypes for numeric columns\")\n",
    "print(\"   - Convert categorical string columns to category dtype\")\n",
    "print(\"   - Consider using sparse matrices for highly sparse data\")\n",
    "\n",
    "print(\"\\n2. Feature Selection:\")\n",
    "print(\"   - Identify and remove redundant or low-importance features\")\n",
    "print(f\"   - For this dataset, the top features are: {', '.join(selected_features)}\")\n",
    "\n",
    "print(\"\\n3. Model Size Considerations:\")\n",
    "print(\"   - Certain models like Random Forests can be memory-intensive\")\n",
    "print(\"   - Consider limiting tree depth or number of estimators if memory is critical\")\n",
    "print(\"   - Simpler models may offer better memory-performance trade-offs\")\n",
    "\n",
    "print(\"\\n4. Memory-Performance Trade-offs:\")\n",
    "print(f\"   - The most memory-efficient approach is {most_efficient_approach['description']}\")\n",
    "print(f\"   - This approach offers {most_efficient_approach['memory_efficiency']:.4f} AUC per MB, with an AUC of {most_efficient_approach['auc']:.4f}\")\n",
    "\n",
    "print(\"\\n5. Implementation Recommendations:\")\n",
    "print(\"   - Implement data type optimization in the preprocessing pipeline\")\n",
    "print(\"   - Use feature selection when loading data for model training/inference\")\n",
    "print(\"   - Consider chunking or batch processing for very large datasets\")\n",
    "print(\"   - Monitor memory usage in production and adjust as needed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
