{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Hyperparameter Tuning for Pediatric Appendicitis Diagnosis (Part 2)\n",
    "\n",
    "This notebook continues our model evaluation, focusing on:\n",
    "- Advanced evaluation metrics for medical diagnosis\n",
    "- Model interpretation and feature importance\n",
    "- Threshold optimization for clinical decision-making\n",
    "- Model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries (similar to Part 1)\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    classification_report, log_loss, brier_score_loss\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data_processing.preprocess import load_data, handle_missing_values, optimize_memory\n",
    "\n",
    "# Set plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data and Best Model from Part 1\n",
    "\n",
    "First, let's load the data and the best tuned model from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data and model...\")\n",
    "\n",
    "# Load synthetic data\n",
    "data_path = '../DATA/synthetic_appendicitis_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[['Age', 'Temperature', 'WBC', 'CRP', 'Pain_Duration', 'Neutrophil_Percent']]\n",
    "y = df['Appendicitis']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Load the scaler\n",
    "try:\n",
    "    scaler = joblib.load('../models/standard_scaler.pkl')\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "except:\n",
    "    print(\"Scaler not found, creating a new one...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Try to load the best model (if it exists)\n",
    "model_dir = Path('../models')\n",
    "model_files = list(model_dir.glob('tuned_*_model.pkl')) if model_dir.exists() else []\n",
    "\n",
    "if model_files:\n",
    "    best_model_path = model_files[0]  # Take the first tuned model file\n",
    "    best_model = joblib.load(best_model_path)\n",
    "    best_model_name = best_model_path.stem.replace('tuned_', '').replace('_model', '').replace('_', ' ').title()\n",
    "    print(f\"Loaded model: {best_model_name} from {best_model_path}\")\n",
    "else:\n",
    "    print(\"No tuned model found, please run Part 1 first or provide a model.\")\n",
    "    # For demonstration, we'll create a simple model\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    best_model = RandomForestClassifier(random_state=42)\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "    best_model_name = \"Random Forest\"\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_prob = best_model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Evaluation Metrics for Medical Diagnosis\n",
    "\n",
    "In medical diagnosis, especially for conditions like appendicitis, we need to consider more specific metrics than just accuracy or AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix elements\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Calculate additional metrics for medical diagnosis\n",
    "sensitivity = tp / (tp + fn)  # Same as Recall\n",
    "specificity = tn / (tn + fp)\n",
    "ppv = tp / (tp + fp)  # Positive Predictive Value, same as Precision\n",
    "npv = tn / (tn + fn)  # Negative Predictive Value\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "f1 = 2 * (ppv * sensitivity) / (ppv + sensitivity)\n",
    "diagnostic_odds_ratio = (tp * tn) / (fp * fn)\n",
    "positive_likelihood_ratio = sensitivity / (1 - specificity)\n",
    "negative_likelihood_ratio = (1 - sensitivity) / specificity\n",
    "\n",
    "# Report clinical metrics\n",
    "print(\"\\nClinical Diagnostic Metrics:\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Positive Predictive Value (Precision): {ppv:.4f}\")\n",
    "print(f\"Negative Predictive Value: {npv:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Diagnostic Odds Ratio: {diagnostic_odds_ratio:.4f}\")\n",
    "print(f\"Positive Likelihood Ratio: {positive_likelihood_ratio:.4f}\")\n",
    "print(f\"Negative Likelihood Ratio: {negative_likelihood_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize advanced metrics\n",
    "metrics = ['Sensitivity', 'Specificity', 'PPV', 'NPV', 'Accuracy', 'F1']\n",
    "values = [sensitivity, specificity, ppv, npv, accuracy, f1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(metrics, values, color=sns.color_palette('viridis', len(metrics)))\n",
    "plt.title('Clinical Evaluation Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add data labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/clinical_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Interpretation and Feature Importance\n",
    "\n",
    "Understanding which features are most important for the model's predictions is crucial for clinical applicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model has feature_importances_ attribute\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # For tree-based models\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance, palette='viridis')\n",
    "    plt.title(f'Feature Importance for {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/feature_importance.png')\n",
    "    plt.show()\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': best_model.coef_[0] if len(best_model.coef_.shape) > 1 else best_model.coef_\n",
    "    }).sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coefficients, palette='viridis')\n",
    "    plt.title(f'Feature Coefficients for {best_model_name}')\n",
    "    plt.axvline(x=0, color='k', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/feature_coefficients.png')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    # For models without direct feature importance, try permutation importance\n",
    "    print(\"This model doesn't have built-in feature importance. Using SHAP values for interpretation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values for model interpretation\n",
    "try:\n",
    "    # Sample for SHAP analysis (using a subset for speed)\n",
    "    X_sample = pd.DataFrame(X_test_scaled[:100], columns=X.columns)\n",
    "    \n",
    "    # Create explainer\n",
    "    explainer = shap.Explainer(best_model, X_sample)\n",
    "    shap_values = explainer(X_sample)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "    plt.title(f'SHAP Feature Importance for {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/shap_importance.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed SHAP plots\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_sample, show=False)\n",
    "    plt.title(f'SHAP Summary Plot for {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/shap_summary.png')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate SHAP plots due to: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Threshold Optimization for Clinical Decision-Making\n",
    "\n",
    "Finding the optimal threshold for clinical decision-making based on cost-benefit considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics at different thresholds\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "threshold_metrics = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_th = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Skip if we have a division by zero issue\n",
    "    if np.sum(y_pred_th) == 0 or np.sum(y_pred_th == 0) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_th).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        f1 = 2 * (ppv * sensitivity) / (ppv + sensitivity) if (ppv + sensitivity) > 0 else 0\n",
    "        \n",
    "        threshold_metrics.append({\n",
    "            'Threshold': threshold,\n",
    "            'Sensitivity': sensitivity,\n",
    "            'Specificity': specificity,\n",
    "            'PPV': ppv,\n",
    "            'NPV': npv,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1': f1\n",
    "        })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "threshold_df = pd.DataFrame(threshold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics vs threshold\n",
    "plt.figure(figsize=(12, 8))\n",
    "metrics_to_plot = ['Sensitivity', 'Specificity', 'PPV', 'NPV', 'Accuracy', 'F1']\n",
    "for metric in metrics_to_plot:\n",
    "    plt.plot(threshold_df['Threshold'], threshold_df[metric], label=metric, linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metrics at Different Classification Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/threshold_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal thresholds for different metrics\n",
    "optimal_thresholds = {\n",
    "    'F1': threshold_df.loc[threshold_df['F1'].idxmax(), 'Threshold'],\n",
    "    'Accuracy': threshold_df.loc[threshold_df['Accuracy'].idxmax(), 'Threshold'],\n",
    "    # Youden's J statistic (Sensitivity + Specificity - 1)\n",
    "    'Youden': threshold_df.loc[(threshold_df['Sensitivity'] + threshold_df['Specificity'] - 1).idxmax(), 'Threshold'],\n",
    "    # Cost-benefit specific threshold for appendicitis\n",
    "    # In medical context for appendicitis, missing a case (false negative) is typically considered worse than\n",
    "    # a false positive, as untreated appendicitis can lead to serious complications\n",
    "    # We could favor sensitivity over specificity with a 2:1 weight ratio\n",
    "    'Clinical': threshold_df.loc[(2*threshold_df['Sensitivity'] + threshold_df['Specificity']).idxmax(), 'Threshold']\n",
    "}\n",
    "\n",
    "print(\"Optimal Thresholds:\")\n",
    "for metric, threshold in optimal_thresholds.items():\n",
    "    print(f\"{metric}: {threshold:.3f}\")\n",
    "\n",
    "# Highlight optimal thresholds on the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "for metric in metrics_to_plot:\n",
    "    plt.plot(threshold_df['Threshold'], threshold_df[metric], label=metric, linewidth=2)\n",
    "\n",
    "# Add vertical lines for optimal thresholds\n",
    "colors = ['r', 'g', 'b', 'm']\n",
    "for i, (metric, threshold) in enumerate(optimal_thresholds.items()):\n",
    "    plt.axvline(x=threshold, color=colors[i], linestyle='--', label=f'{metric} Optimal: {threshold:.3f}')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Optimal Classification Thresholds for Different Criteria')\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/optimal_thresholds.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
